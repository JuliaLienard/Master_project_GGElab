# -*- coding: utf-8 -*-
#!/usr/bin/ python3
"""
Author : Julia Lienard 
Date : 2025/07/02

Description: the script creates a transcript expression matrix that can be used for tappAS.
The necessary input files are:
    1) the output file from tama merge <prefix>_trans_report.txt
    /home/jlienard/BetaCells_kinnex/03_collapsing_isoseq_collapse/02_Using_GRCh38_mapping/output/01_tama_merge
    2) the abundance files generated by isoseq collapse for all samples that are merged with tama merge. From these files, 
    the number of fl reads per transcript can be found (linking PB transcript ID, FL counts and tama transcript ID)
    found on garnatxa /home/jlienard/BetaCells_kinnex/03_collapsing_isoseq_collapse/02_Using_GRCh38_mapping/output

Procedure:
    1) <prefix>_trans_report.txt is parsed to create a dictionary with tama transcripts ID as the key and the
    original PacBio transcript ID for each sample as the values. The dictionary is output as a temporary file
    tama_PacBio_ID.txt
    2) For each sample abundance file, a dictionary is created to collect PacBio transcript ID and 
    FL count (Full length read counts). The fl_assoc column of the abundance file is used (correct fL counts)
    3) The temporary file tama_PacBio_ID.txt is completed with a column for the corresponding fl_count for 
    each of the two samples

limitations: this script can be used when only 2 samples were merged with tama merge, and need to be modified if more samples were used

Usage: python transcription_expression_matrix.py <prefix>_trans_report.txt sample1_abundance.tsv sample2_abundance.tsv prefix_matrix
"""
import os
import sys

trans_report_file = sys.argv[1]
abundance1 = sys.argv[2]
abundance2 = sys.argv[3]
prefix_matrix = sys.argv[4]

# collecting the tama merge transcript ID and corresponding PacBio transcript ID 
# from the prefix_trans_report.txt which is an output from tama merge
with open(trans_report_file, "r") as trans_report, open("tama_PacBio_ID.txt", "w") as output1:
    tama_dict = {} # collect the tama ID and corresponding PB ID for the two samples into a dictionary
    for line in trans_report:
        if line.startswith("G"): # all tama ID start by G
            row = line.strip().split("\t")
            tama_id = row[0]
            nb_cluster = row[1]
            source = row[2]
            PB_ID_all = row[7].split(",")

            # Initialize default values
            cleanPB_id_bc01 = 0
            cleanPB_id_bc02 = 0

            # Loop through all PB IDs to find and assign each source
            for pb in PB_ID_all:
                if pb.startswith("bc01"):
                    cleanPB_id_bc01 = pb.split("_")[1]
                elif pb.startswith("bc02"):
                    cleanPB_id_bc02 = pb.split("_")[1]

            # Save both values regardless of their order in the input
            tama_dict[tama_id] = (cleanPB_id_bc01, cleanPB_id_bc02)
                
    for tama_id, (cleanPB_id_bc01, cleanPB_id_bc02) in tama_dict.items():
        output1.write(f"{tama_id}\t{cleanPB_id_bc01}\t{cleanPB_id_bc02}\n")
    
    # checking that no similar PacBio ID are attributed to different genes by tama merge:
    PB_bc01_list = [tama_id for tama_id, values in tama_dict.items() if values[0] != 0] # collect all values
    PB_bc01_set = (tama_id for tama_id, values in tama_dict.items() if values[0] != 0) # collect only unique values
    if len(list(PB_bc01_list)) != len(set(PB_bc01_set)):
        print("Warning: duplicated PacBio ID for bc01")
    
    PB_bc02_list = [tama_id for tama_id, values in tama_dict.items() if values[1] != 0]
    PB_bc02_set = (tama_id for tama_id, values in tama_dict.items() if values[1] != 0)
    if len(list(PB_bc02_list)) != len(set(PB_bc02_set)):
        print("Warning: duplicated PacBio ID for bc02")
 
# Collecting the fl counts for sample bc01 from the isoseq collapse abundance.tsv file  
fl_count_dict1 = {}
with open(abundance1, "r") as bc01_FL:
    for line1 in bc01_FL:
        if line1.startswith("PB"):
            rowFL1 = line1.strip().split("\t")
            pbid_bc01 = rowFL1[0]
            fl_count_bc01 = rowFL1[3]
            fl_count_dict1[pbid_bc01] = fl_count_bc01
            
with open("tama_PacBio_ID.txt", "r") as templist, open("tama_PacBio_ID_FL.txt", "w") as output2:
    for line_templist in templist:
        row_templist = line_templist.strip().split("\t")
        tama_id, PB_id_bc01, PB_id_bc02 = row_templist  # Extract columns
        # Look up FL count from bc01_FL dictionary
        fl_count_bc01 = fl_count_dict1.get(PB_id_bc01, "0") 
        # Write to output file
        output2.write(f"{tama_id}\t{PB_id_bc01}\t{PB_id_bc02}\t{fl_count_bc01}\n")

# Collecting the fl counts for sample bc02 from the isoseq collapse abundance.tsv file  
fl_count_dict2 = {}
with open(abundance2, "r") as bc02_FL:
    for line2 in bc02_FL:
        if line2.startswith("PB"):
            rowFL2 = line2.strip().split("\t")
            pbid_bc02 = rowFL2[0]
            fl_count_bc02 = rowFL2[3]
            fl_count_dict2[pbid_bc02] = fl_count_bc02
            
with open("tama_PacBio_ID_FL.txt", "r") as templist2, open(f"{prefix_matrix}_tama_PacBio_ID_FL.txt", "w") as output3,\
    open(f"{prefix_matrix}_transcript_expression_matrix_replicates.txt", "w") as matrix:
    output3.write('tama_id\tPB_id_bc01\tPB_id_bc02\tfl_count_bc01\tfl_count_bc02\n') # header
    matrix.write('\tbc01\tbc01_10\tbc02\tbc02_10\n') # the goal is to use the matrix in tappAS that only allows groups with duplicates
    for line_templist2 in templist2:
        row_templist2 = line_templist2.strip().split("\t")
        tama_id, PB_id_bc01, PB_id_bc02, fl_count_bc01 = row_templist2  # Extract columns
        # Look up FL count from bc01_FL dictionary
        fl_count_bc02 = fl_count_dict2.get(PB_id_bc02, "0") 
        # Write to output file
        output3.write(f"{tama_id}\t{PB_id_bc01}\t{PB_id_bc02}\t{fl_count_bc01}\t{fl_count_bc02}\n")
        matrix.write(f"{tama_id}\t{int(fl_count_bc01)}\t{int(int(fl_count_bc01)*0.9)}\t{int(fl_count_bc02)}\t{int(int(fl_count_bc02)*0.9)}\n")

# remove temporary files
os.remove("tama_PacBio_ID_FL.txt")
os.remove("tama_PacBio_ID.txt")